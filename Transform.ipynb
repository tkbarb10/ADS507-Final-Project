{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import mysql.connector as mycon\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This pulls in the wildfire data and reads it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:91: DtypeWarning: Columns (14,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = read_function(\n"
     ]
    }
   ],
   "source": [
    "use_cols = ['OBJECTID', 'FIRE_NAME', 'DISCOVERY_DATE', 'NWCG_GENERAL_CAUSE', 'FIRE_SIZE', 'STATE', 'FIPS_NAME']\n",
    "\n",
    "# Download latest version\n",
    "wildfire_df = kagglehub.load_dataset(handle = \"behroozsohrabi/us-wildfire-records-6th-edition\", path = \"data.csv\", \n",
    "                                   adapter = KaggleDatasetAdapter.PANDAS, \n",
    "                                   pandas_kwargs={\"usecols\": use_cols, \"compression\": \"zip\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DISCOVERY_DATE to datetime object to remove records prior to year 2000\n",
    "\n",
    "wildfire_df['DISCOVERY_DATE'] = pd.to_datetime(wildfire_df['DISCOVERY_DATE'], format = ('%m/%d/%Y'))\n",
    "\n",
    "wildfire_df = wildfire_df[wildfire_df['DISCOVERY_DATE'].dt.year > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns for clarity\n",
    "\n",
    "wildfire_df = wildfire_df.rename(columns = {\"NWCG_GENERAL_CAUSE\": \"SPECIFIC_CAUSE\", \"FIPS_NAME\": \"COUNTY\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter states that we're keeping\n",
    "\n",
    "states_to_keep = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "wildfire_df.loc[~wildfire_df['STATE'].isin(states_to_keep), :] = None\n",
    "\n",
    "wildfire_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to int for compatiabilty with Primary Key\n",
    "\n",
    "wildfire_df['OBJECTID'] = wildfire_df['OBJECTID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This pulls in housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_tier_housing = pd.read_parquet('data/bottom_housing.parquet')\n",
    "top_tier_housing = pd.read_parquet('data/top_housing.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns we don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "\n",
    "bottom_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "top_tier_housing.drop(columns = remove_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with states we aren't using in a way that modifies the original dataframe in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "bottom_tier_housing.loc[~bottom_tier_housing['State'].isin(states), :] = None\n",
    "top_tier_housing.loc[~top_tier_housing['State'].isin(states), :] = None\n",
    "\n",
    "bottom_tier_housing.dropna(inplace = True)\n",
    "top_tier_housing.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data frames and pivot long the dates and prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.concat([bottom_tier_housing, top_tier_housing], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "housing_df['Date'] = pd.to_datetime(housing_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rent Data (load and remove columns'state values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_index = pd.read_csv(\"data/Observed Rent Index by City.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and state values to remove are same as housing sales data\n",
    "\n",
    "rent_index.drop(columns = remove_cols, inplace = True)\n",
    "rent_index.loc[~rent_index['State'].isin(states), :] = None\n",
    "\n",
    "rent_index.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot long the date and price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_index = rent_index.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "rent_index['Date'] = pd.to_datetime(housing_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Location Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in location info from Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st06_ca_place_by_county2020.txt', delimiter = '|')\n",
    "tx_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st48_tx_place_by_county2020.txt', delimiter = '|')\n",
    "ga_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st13_ga_place_by_county2020.txt', delimiter = '|')\n",
    "fl_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st12_fl_place_by_county2020.txt', delimiter = '|')\n",
    "az_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st04_az_place_by_county2020.txt', delimiter = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PLACENS', 'TYPE', 'CLASSFP', 'FUNCSTAT']\n",
    "\n",
    "ca_codes.drop(columns = columns_to_drop, inplace = True)\n",
    "tx_codes.drop(columns = columns_to_drop, inplace = True)\n",
    "ga_codes.drop(columns = columns_to_drop, inplace = True)\n",
    "fl_codes.drop(columns = columns_to_drop, inplace = True)\n",
    "az_codes.drop(columns = columns_to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df to make it easier to load\n",
    "\n",
    "combined_df = pd.concat([ca_codes, tx_codes, ga_codes, fl_codes, az_codes], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Population Data into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population_dict = {}\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "for state in states:\n",
    "    state_population_dict[state] = pd.read_csv(f\"data/{state} City population estimates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync population and location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dict = {\n",
    "    'CA': ca_codes,\n",
    "    'TX': tx_codes,\n",
    "    'GA': ga_codes,\n",
    "    'FL': fl_codes,\n",
    "    'AZ': az_codes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync(states_list):\n",
    "    for state in states_list:\n",
    "        pop_df = state_population_dict[state]\n",
    "        location_df = location_dict[state]\n",
    "\n",
    "        columns_to_rename = {'PLACE': 'PLACEFP', 'NAME': 'PLACENAME'}\n",
    "\n",
    "        pop_df = pop_df.rename(columns = columns_to_rename)\n",
    "\n",
    "        merged_df = pop_df.merge(\n",
    "            location_df[['COUNTYFP', 'COUNTYNAME', 'PLACEFP', 'PLACENAME']],\n",
    "            on = [\"PLACEFP\", 'PLACENAME'],\n",
    "            how = 'left'\n",
    "        )\n",
    "        merged_df.drop(columns = ['SUMLEV', 'COUSUB', 'CONCIT', 'PRIMGEO_FLAG', 'FUNCSTAT', 'ESTIMATESBASE2010', 'COUNTY'], inplace = True)\n",
    "        merged_df = merged_df.drop_duplicates()\n",
    "        merged_df = merged_df.dropna()\n",
    "        merged_df['COUNTYFP'] = merged_df['COUNTYFP'].astype(int)\n",
    "        state_population_dict[state] = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map state and county codes to wildfire/housing/rent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping only for CA, TX, FL, GA, and AZ\n",
    "\n",
    "state_fips_mapping = {\n",
    "    \"CA\": 6,  # California\n",
    "    \"TX\": 48, # Texas\n",
    "    \"FL\": 12, # Florida\n",
    "    \"GA\": 13, # Georgia\n",
    "    \"AZ\": 4   # Arizona\n",
    "}\n",
    "\n",
    "wildfire_df['state_id'] = wildfire_df['STATE'].map(state_fips_mapping)\n",
    "housing_df['state_id'] = housing_df['State'].map(state_fips_mapping)\n",
    "rent_index['state_id'] = rent_index['State'].map(state_fips_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realign column names to prepare for county code mapping\n",
    "\n",
    "combined_df.rename(columns = {'COUNTYNAME': 'county_name', 'COUNTYFP': 'county_id', 'STATEFP': 'state_id'}, inplace = True)\n",
    "rent_index.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "housing_df.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "wildfire_df.rename(columns = {'COUNTY': 'county_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz  # Much faster than thefuzz\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "def match_county_id(data_df, counties_df, state_col=\"state_id\", county_col=\"county_name\", threshold=85):\n",
    "    \"\"\"\n",
    "    Matches county names in the dataset to official counties using fuzzy matching.\n",
    "\n",
    "    Optimizations:\n",
    "    - Uses `rapidfuzz` (faster fuzzy matching)\n",
    "    - Caches results to avoid redundant computations\n",
    "    - Precomputes county name choices per state\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): DataFrame containing records that need county_id assignment.\n",
    "    - counties_df (pd.DataFrame): Official Census counties with state_id, county_name, and county_id.\n",
    "    - state_col (str): Column name for state_id.\n",
    "    - county_col (str): Column name for county_name.\n",
    "    - threshold (int): Minimum similarity score to consider a match.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with county_id assigned.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.pandas()  # Enable progress bar\n",
    "    \n",
    "    # Create lookup dictionary: {state_id: (county_name_list, county_id_map)}\n",
    "    county_lookup = {\n",
    "        state_id: (\n",
    "            counties_df[counties_df[state_col] == state_id][\"county_name\"].tolist(),\n",
    "            counties_df[counties_df[state_col] == state_id].set_index(\"county_name\")[\"county_id\"].to_dict()\n",
    "        )\n",
    "        for state_id in counties_df[state_col].unique()\n",
    "    }\n",
    "\n",
    "    # Cache for already matched county names\n",
    "    match_cache = {}\n",
    "\n",
    "    def find_best_match(state_id, county_name):\n",
    "        if state_id in county_lookup:\n",
    "            county_names, county_id_map = county_lookup[state_id]\n",
    "\n",
    "            # Check cache first\n",
    "            cache_key = (state_id, county_name)\n",
    "            if cache_key in match_cache:\n",
    "                return match_cache[cache_key]\n",
    "\n",
    "            # Fuzzy match\n",
    "            match, score, _ = process.extractOne(county_name, county_names, scorer=fuzz.token_sort_ratio)\n",
    "            if score >= threshold:\n",
    "                county_id = county_id_map[match]\n",
    "                match_cache[cache_key] = county_id  # Cache result\n",
    "                return county_id\n",
    "\n",
    "        return None  # No match found\n",
    "\n",
    "    # Apply fuzzy matching with caching and progress bar\n",
    "    data_df[\"county_id\"] = data_df.progress_apply(lambda row: find_best_match(row[state_col], row[county_col]), axis=1)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35760 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35760/35760 [00:01<00:00, 28064.08it/s]\n"
     ]
    }
   ],
   "source": [
    "rent_index = match_county_id(rent_index, combined_df)\n",
    "housing_df = match_county_id(housing_df, combined_df)\n",
    "wildfire_df = match_county_id(wildfire_df, combined_df)\n",
    "population_df = pd.concat(state_population_dict.values(), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of missing values in wildfire dataframe, fill in variables accordingly\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    Fill missing values based on the state column.\n",
    "    \"\"\"\n",
    "    df.loc[df['STATE'] == 'FL', 'county_id'] = df.loc[df['STATE'] == 'FL', 'county_id'].fillna(86)\n",
    "    df.loc[df['STATE'] == 'GA', 'county_id'] = df.loc[df['STATE'] == 'GA', 'county_id'].fillna(29)\n",
    "    return df\n",
    "\n",
    "wildfire_df = fill_missing_values(wildfire_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df['CENSUS2010POP'] = population_df['CENSUS2010POP'].apply(lambda x: 0 if x == 'A' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final dataframes\n",
    "\n",
    "rent_index  \n",
    "housing_df  \n",
    "wildfire_df  \n",
    "combined_df  \n",
    "population_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as mysql\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = 'wildfire-watch1.mysql.database.azure.com'\n",
    "user = 'tkirk'\n",
    "port = 3306\n",
    "password = 'Wildfire507'\n",
    "database = 'wildfire_housing'\n",
    "ssl = \"/DigiCertGlobalRootCA.crt (1).pem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reconnect to server in case of timeout error\n",
    "\n",
    "def reconnect():\n",
    "\n",
    "    global conn, cursor\n",
    "\n",
    "    try:\n",
    "        conn = mysql.connect(\n",
    "            host = server_name,\n",
    "            user = user,\n",
    "            port = port,\n",
    "            password = password,\n",
    "            database = database,\n",
    "            ssl_ca = ssl\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Reconnected\")\n",
    "\n",
    "    except Error as err:\n",
    "        print(f\"❌ Connection unsuccessful {err}\")\n",
    "        time.sleep(3)\n",
    "        reconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load locations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "columns = combined_df.columns\n",
    "\n",
    "data_to_insert = list(combined_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "insert_clause = \"\"\"\n",
    "INSERT INTO locations (state_name, state_id, county_id, county_name, place_id, place_name)\n",
    "Values (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(insert_clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load wildfire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "columns = wildfire_df.columns\n",
    "\n",
    "data_to_insert = list(wildfire_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT INTO wildfire (fire_id, fire_name, discovery_date, cause, fire_size, state_name, county_name, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10000\n",
    "start_index = 140000\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        cursor.executemany(clause, batch)\n",
    "        conn.commit()\n",
    "        print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully for batch 17\n",
      "Data loaded successfully for batch 18\n",
      "Data loaded successfully for batch 19\n",
      "Data loaded successfully for batch 20\n",
      "Data loaded successfully for batch 21\n",
      "Data loaded successfully for batch 22\n",
      "Data loaded successfully for batch 23\n",
      "Data loaded successfully for batch 24\n",
      "Data loaded successfully for batch 25\n",
      "Data loaded successfully for batch 26\n",
      "Data loaded successfully for batch 27\n",
      "Data loaded successfully for batch 28\n",
      "Data loaded successfully for batch 29\n",
      "Data loaded successfully for batch 30\n",
      "Data loaded successfully for batch 31\n",
      "Data loaded successfully for batch 32\n",
      "Data loaded successfully for batch 33\n",
      "Data loaded successfully for batch 34\n",
      "Data loaded successfully for batch 35\n",
      "Data loaded successfully for batch 36\n",
      "Data loaded successfully for batch 37\n",
      "Data loaded successfully for batch 38\n",
      "Data loaded successfully for batch 39\n",
      "Data loaded successfully for batch 40\n",
      "Data loaded successfully for batch 41\n",
      "Data loaded successfully for batch 42\n",
      "Data loaded successfully for batch 43\n",
      "Data loaded successfully for batch 44\n",
      "Data loaded successfully for batch 45\n",
      "Data loaded successfully for batch 46\n",
      "Data loaded successfully for batch 47\n",
      "Data loaded successfully for batch 48\n",
      "Data loaded successfully for batch 49\n",
      "Data loaded successfully for batch 50\n",
      "Data loaded successfully for batch 51\n",
      "Data loaded successfully for batch 52\n",
      "Data loaded successfully for batch 53\n",
      "Data loaded successfully for batch 54\n",
      "Data loaded successfully for batch 55\n",
      "Data loaded successfully for batch 56\n",
      "Data loaded successfully for batch 57\n",
      "Data loaded successfully for batch 58\n",
      "Data loaded successfully for batch 59\n",
      "Data loaded successfully for batch 60\n",
      "Data loaded successfully for batch 61\n",
      "Data loaded successfully for batch 62\n",
      "Data loaded successfully for batch 63\n",
      "Data loaded successfully for batch 64\n",
      "Data loaded successfully for batch 65\n",
      "Data loaded successfully for batch 66\n",
      "Data loaded successfully for batch 67\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "columns = housing_df.columns\n",
    "\n",
    "data_to_insert = list(housing_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT INTO housing (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == '2013 (HY000): Lost connection to MySQL server during query':\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)  # Retry query\n",
    "                conn.commit()\n",
    "\n",
    "except Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully for batch 1\n",
      "Data loaded successfully for batch 2\n",
      "Data loaded successfully for batch 3\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "columns = rent_index.columns\n",
    "\n",
    "data_to_insert = list(rent_index[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT INTO rentals (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == \"2013 (HY000): Lost connection to MySQL server during query\":\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)\n",
    "                conn.commmit()\n",
    "except Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "data_to_insert = list(population_df[['STATE', 'COUNTYFP', 'PLACEFP', 'CENSUS2010POP', 'POPESTIMATE2011', 'POPESTIMATE2012', \n",
    "                                     'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015','POPESTIMATE2016', 'POPESTIMATE2017', \n",
    "                                     'POPESTIMATE2018','POPESTIMATE2019']].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO census (state_id, county_id, place_id, true_pop_2010, pop_estimate_2011, pop_estimate_2012, pop_estimate_2013, pop_estimate_2014, \n",
    "pop_estimate_2015, pop_estimate_2016, pop_estimate_2017, pop_estimate_2018, pop_estimate_2019)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data_to_insert)):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        cursor.executemany(clause, batch)\n",
    "        conn.commit()\n",
    "        print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
